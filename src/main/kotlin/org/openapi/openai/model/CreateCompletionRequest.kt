package org.openapi.openai.model

import com.fasterxml.jackson.annotation.JsonProperty
import io.swagger.v3.oas.annotations.media.Schema
import jakarta.annotation.Generated
import jakarta.validation.Valid
import jakarta.validation.constraints.DecimalMax
import jakarta.validation.constraints.DecimalMin
import jakarta.validation.constraints.Max
import jakarta.validation.constraints.Min
import org.openapitools.jackson.nullable.JsonNullable
import java.math.BigDecimal
import java.util.*

/**
 * CreateCompletionRequest
 */
@Generated(
    value = ["org.openapitools.codegen.languages.SpringCodegen"],
    date = "2023-02-15T20:47:29.471771-08:00[America/Vancouver]"
)
class CreateCompletionRequest {
    /**
     * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.
     * @return model
     */
    @get:Schema(
        name = "model",
        description = "ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.",
        requiredMode = Schema.RequiredMode.REQUIRED
    )
    @JsonProperty("model")
    var model: String? = null

    @JsonProperty("prompt")
    private var prompt = JsonNullable.undefined<CreateCompletionRequestPrompt>()

    /**
     * The suffix that comes after a completion of inserted text.
     * @return suffix
     */
    @get:Schema(
        name = "suffix",
        example = "test.",
        description = "The suffix that comes after a completion of inserted text.",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    @JsonProperty("suffix")
    var suffix = JsonNullable.undefined<String>()

    @JsonProperty("max_tokens")
    private var maxTokens = JsonNullable.undefined<Int>()

    @JsonProperty("temperature")
    private var temperature = JsonNullable.undefined<BigDecimal>()

    @JsonProperty("top_p")
    private var topP = JsonNullable.undefined<BigDecimal>()

    @JsonProperty("n")
    private var n = JsonNullable.undefined<Int>()

    /**
     * Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.
     * @return stream
     */
    @get:Schema(
        name = "stream",
        description = "Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    @JsonProperty("stream")
    var stream = JsonNullable.undefined<Boolean>()

    @JsonProperty("logprobs")
    private var logprobs = JsonNullable.undefined<Int>()

    /**
     * Echo back the prompt in addition to the completion
     * @return echo
     */
    @get:Schema(
        name = "echo",
        description = "Echo back the prompt in addition to the completion ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    @JsonProperty("echo")
    var echo = JsonNullable.undefined<Boolean>()

    @JsonProperty("stop")
    private var stop = JsonNullable.undefined<CreateCompletionRequestStop>()

    @JsonProperty("presence_penalty")
    private var presencePenalty = JsonNullable.undefined<BigDecimal>()

    @JsonProperty("frequency_penalty")
    private var frequencyPenalty = JsonNullable.undefined<BigDecimal>()

    @JsonProperty("best_of")
    private var bestOf = JsonNullable.undefined<Int>()

    /**
     * Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.
     * @return logitBias
     */
    @get:Schema(
        name = "logit_bias",
        description = "Modify the likelihood of specified tokens appearing in the completion.  Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.  As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated. ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    @JsonProperty("logit_bias")
    var logitBias = JsonNullable.undefined<Any>()

    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     * @return user
     */
    @get:Schema(
        name = "user",
        example = "user-1234",
        description = "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    @JsonProperty("user")
    var user: String? = null
    fun model(model: String?): CreateCompletionRequest {
        this.model = model
        return this
    }

    fun prompt(prompt: CreateCompletionRequestPrompt): CreateCompletionRequest {
        this.prompt = JsonNullable.of(prompt)
        return this
    }

    /**
     * Get prompt
     * @return prompt
     */
    @Schema(name = "prompt", requiredMode = Schema.RequiredMode.NOT_REQUIRED)
    fun getPrompt(): @Valid JsonNullable<CreateCompletionRequestPrompt>? {
        return prompt
    }

    fun setPrompt(prompt: JsonNullable<CreateCompletionRequestPrompt>) {
        this.prompt = prompt
    }

    fun suffix(suffix: String): CreateCompletionRequest {
        this.suffix = JsonNullable.of(suffix)
        return this
    }

    fun maxTokens(maxTokens: Int): CreateCompletionRequest {
        this.maxTokens = JsonNullable.of(maxTokens)
        return this
    }

    /**
     * The maximum number of [tokens](/tokenizer) to generate in the completion.  The token count of your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
     * minimum: 0
     * @return maxTokens
     */
    @Schema(
        name = "max_tokens",
        example = "16",
        description = "The maximum number of [tokens](/tokenizer) to generate in the completion.  The token count of your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getMaxTokens(): @Min(0) JsonNullable<Int>? {
        return maxTokens
    }

    fun setMaxTokens(maxTokens: JsonNullable<Int>) {
        this.maxTokens = maxTokens
    }

    fun temperature(temperature: BigDecimal): CreateCompletionRequest {
        this.temperature = JsonNullable.of(temperature)
        return this
    }

    /**
     * What [sampling temperature](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277) to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.  We generally recommend altering this or `top_p` but not both.
     * minimum: 0
     * maximum: 2
     * @return temperature
     */
    @Schema(
        name = "temperature",
        example = "1",
        description = "What [sampling temperature](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277) to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.  We generally recommend altering this or `top_p` but not both. ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getTemperature(): @Valid @DecimalMin("0") @DecimalMax("2") JsonNullable<BigDecimal>? {
        return temperature
    }

    fun setTemperature(temperature: JsonNullable<BigDecimal>) {
        this.temperature = temperature
    }

    fun topP(topP: BigDecimal): CreateCompletionRequest {
        this.topP = JsonNullable.of(topP)
        return this
    }

    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both.
     * minimum: 0
     * maximum: 1
     * @return topP
     */
    @Schema(
        name = "top_p",
        example = "1",
        description = "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both. ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getTopP(): @Valid @DecimalMin("0") @DecimalMax("1") JsonNullable<BigDecimal>? {
        return topP
    }

    fun setTopP(topP: JsonNullable<BigDecimal>) {
        this.topP = topP
    }

    fun n(n: Int): CreateCompletionRequest {
        this.n = JsonNullable.of(n)
        return this
    }

    /**
     * How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
     * minimum: 1
     * maximum: 128
     * @return n
     */
    @Schema(
        name = "n",
        example = "1",
        description = "How many completions to generate for each prompt.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getN(): @Min(1) @Max(128) JsonNullable<Int>? {
        return n
    }

    fun setN(n: JsonNullable<Int>) {
        this.n = n
    }

    fun stream(stream: Boolean): CreateCompletionRequest {
        this.stream = JsonNullable.of(stream)
        return this
    }

    fun logprobs(logprobs: Int): CreateCompletionRequest {
        this.logprobs = JsonNullable.of(logprobs)
        return this
    }

    /**
     * Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.  The maximum value for `logprobs` is 5. If you need more than this, please contact us through our [Help center](https://help.openai.com) and describe your use case.
     * minimum: 0
     * maximum: 5
     * @return logprobs
     */
    @Schema(
        name = "logprobs",
        description = "Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.  The maximum value for `logprobs` is 5. If you need more than this, please contact us through our [Help center](https://help.openai.com) and describe your use case. ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getLogprobs(): @Min(0) @Max(5) JsonNullable<Int>? {
        return logprobs
    }

    fun setLogprobs(logprobs: JsonNullable<Int>) {
        this.logprobs = logprobs
    }

    fun echo(echo: Boolean): CreateCompletionRequest {
        this.echo = JsonNullable.of(echo)
        return this
    }

    fun stop(stop: CreateCompletionRequestStop): CreateCompletionRequest {
        this.stop = JsonNullable.of(stop)
        return this
    }

    /**
     * Get stop
     * @return stop
     */
    @Schema(name = "stop", requiredMode = Schema.RequiredMode.NOT_REQUIRED)
    fun getStop(): @Valid JsonNullable<CreateCompletionRequestStop>? {
        return stop
    }

    fun setStop(stop: JsonNullable<CreateCompletionRequestStop>) {
        this.stop = stop
    }

    fun presencePenalty(presencePenalty: BigDecimal): CreateCompletionRequest {
        this.presencePenalty = JsonNullable.of(presencePenalty)
        return this
    }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
     * minimum: -2
     * maximum: 2
     * @return presencePenalty
     */
    @Schema(
        name = "presence_penalty",
        description = "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getPresencePenalty(): @Valid @DecimalMin("-2") @DecimalMax("2") JsonNullable<BigDecimal>? {
        return presencePenalty
    }

    fun setPresencePenalty(presencePenalty: JsonNullable<BigDecimal>) {
        this.presencePenalty = presencePenalty
    }

    fun frequencyPenalty(frequencyPenalty: BigDecimal): CreateCompletionRequest {
        this.frequencyPenalty = JsonNullable.of(frequencyPenalty)
        return this
    }

    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)
     * minimum: -2
     * maximum: 2
     * @return frequencyPenalty
     */
    @Schema(
        name = "frequency_penalty",
        description = "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.  [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getFrequencyPenalty(): @Valid @DecimalMin("-2") @DecimalMax("2") JsonNullable<BigDecimal>? {
        return frequencyPenalty
    }

    fun setFrequencyPenalty(frequencyPenalty: JsonNullable<BigDecimal>) {
        this.frequencyPenalty = frequencyPenalty
    }

    fun bestOf(bestOf: Int): CreateCompletionRequest {
        this.bestOf = JsonNullable.of(bestOf)
        return this
    }

    /**
     * Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.  When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return – `best_of` must be greater than `n`.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
     * minimum: 0
     * maximum: 20
     * @return bestOf
     */
    @Schema(
        name = "best_of",
        description = "Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.  When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return – `best_of` must be greater than `n`.  **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`. ",
        requiredMode = Schema.RequiredMode.NOT_REQUIRED
    )
    fun getBestOf(): @Min(0) @Max(20) JsonNullable<Int>? {
        return bestOf
    }

    fun setBestOf(bestOf: JsonNullable<Int>) {
        this.bestOf = bestOf
    }

    fun logitBias(logitBias: Any): CreateCompletionRequest {
        this.logitBias = JsonNullable.of(logitBias)
        return this
    }

    fun user(user: String?): CreateCompletionRequest {
        this.user = user
        return this
    }

    override fun equals(other: Any?): Boolean {
        if (this === other) {
            return true
        }
        if (other == null || javaClass != other.javaClass) {
            return false
        }
        val createCompletionRequest = other as CreateCompletionRequest
        return model == createCompletionRequest.model &&
                equalsNullable(prompt, createCompletionRequest.prompt) &&
                equalsNullable(suffix, createCompletionRequest.suffix) &&
                equalsNullable(maxTokens, createCompletionRequest.maxTokens) &&
                equalsNullable(temperature, createCompletionRequest.temperature) &&
                equalsNullable(topP, createCompletionRequest.topP) &&
                equalsNullable(n, createCompletionRequest.n) &&
                equalsNullable(stream, createCompletionRequest.stream) &&
                equalsNullable(logprobs, createCompletionRequest.logprobs) &&
                equalsNullable(echo, createCompletionRequest.echo) &&
                equalsNullable(stop, createCompletionRequest.stop) &&
                equalsNullable(presencePenalty, createCompletionRequest.presencePenalty) &&
                equalsNullable(frequencyPenalty, createCompletionRequest.frequencyPenalty) &&
                equalsNullable(bestOf, createCompletionRequest.bestOf) &&
                equalsNullable(logitBias, createCompletionRequest.logitBias) && user == createCompletionRequest.user
    }

    override fun hashCode(): Int {
        return Objects.hash(
            model,
            hashCodeNullable(prompt),
            hashCodeNullable(suffix),
            hashCodeNullable(maxTokens),
            hashCodeNullable(temperature),
            hashCodeNullable(topP),
            hashCodeNullable(n),
            hashCodeNullable(stream),
            hashCodeNullable(logprobs),
            hashCodeNullable(echo),
            hashCodeNullable(stop),
            hashCodeNullable(presencePenalty),
            hashCodeNullable(frequencyPenalty),
            hashCodeNullable(bestOf),
            hashCodeNullable(logitBias),
            user
        )
    }

    override fun toString(): String {
        val sb = StringBuilder()
        sb.append("class CreateCompletionRequest {\n")
        sb.append("    model: ").append(toIndentedString(model)).append("\n")
        sb.append("    prompt: ").append(toIndentedString(prompt)).append("\n")
        sb.append("    suffix: ").append(toIndentedString(suffix)).append("\n")
        sb.append("    maxTokens: ").append(toIndentedString(maxTokens)).append("\n")
        sb.append("    temperature: ").append(toIndentedString(temperature)).append("\n")
        sb.append("    topP: ").append(toIndentedString(topP)).append("\n")
        sb.append("    n: ").append(toIndentedString(n)).append("\n")
        sb.append("    stream: ").append(toIndentedString(stream)).append("\n")
        sb.append("    logprobs: ").append(toIndentedString(logprobs)).append("\n")
        sb.append("    echo: ").append(toIndentedString(echo)).append("\n")
        sb.append("    stop: ").append(toIndentedString(stop)).append("\n")
        sb.append("    presencePenalty: ").append(toIndentedString(presencePenalty)).append("\n")
        sb.append("    frequencyPenalty: ").append(toIndentedString(frequencyPenalty)).append("\n")
        sb.append("    bestOf: ").append(toIndentedString(bestOf)).append("\n")
        sb.append("    logitBias: ").append(toIndentedString(logitBias)).append("\n")
        sb.append("    user: ").append(toIndentedString(user)).append("\n")
        sb.append("}")
        return sb.toString()
    }

    /**
     * Convert the given object to string with each line indented by 4 spaces
     * (except the first line).
     */
    private fun toIndentedString(o: Any?): String {
        return o?.toString()?.replace("\n", "\n    ") ?: "null"
    }

    companion object {
        private fun <T> equalsNullable(a: JsonNullable<T>?, b: JsonNullable<T>?): Boolean {
            return a === b || a != null && b != null && a.isPresent && b.isPresent && Objects.deepEquals(
                a.get(),
                b.get()
            )
        }

        private fun <T : Any> hashCodeNullable(a: JsonNullable<T>?): Int {
            if (a == null) {
                return 1
            }
            return if (a.isPresent) arrayOf<Any>(a.get()).contentDeepHashCode() else 31
        }
    }
}